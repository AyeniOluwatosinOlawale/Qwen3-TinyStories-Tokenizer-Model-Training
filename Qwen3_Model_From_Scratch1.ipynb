{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G27G6lrwcgY8"
   },
   "source": [
    "# Qwen3 TinyStories Tokenizer & Model Training\n",
    "\n",
    "## Overview\n",
    "This notebook implements a complete Qwen3-based language model training pipeline using the TinyStories dataset. It includes tokenization, model architecture, and training with optimized configurations.\n",
    "\n",
    "## Features\n",
    "- **Qwen3 Tokenizer**: Latest tokenizer with fallback to Qwen2.5\n",
    "- **Custom Qwen3 Architecture**: Grouped Query Attention, SwiGLU, RMSNorm\n",
    "- **Optimized Training**: Mixed precision, gradient accumulation, learning rate scheduling\n",
    "- **Memory Efficient**: Optimized batch loading and processing\n",
    "- **Model Generation**: Built-in text generation capabilities\n",
    "\n",
    "## Model Configuration\n",
    "- **Parameters**: ~283M (optimized for efficiency)\n",
    "- **Architecture**: Qwen3 with GQA, SwiGLU, RoPE\n",
    "- **Context Length**: 32768 tokens\n",
    "- **Training**: 150k iterations with cosine decay\n",
    "\n",
    "## Usage\n",
    "1. Run all cells to train the model\n",
    "2. Model will be saved as `qwen3_slm.pt`\n",
    "3. Use the generation section to test the trained model\n",
    "4. Training progress and loss plots are automatically generated\n",
    "\n",
    "## Performance\n",
    "- **Training Time**: ~5-6 hours on A4 GPU\n",
    "- **Memory Usage**: ~8GB VRAM\n",
    "- **Final Loss**: Typically 2.5-3.0 on TinyStories\n",
    "\n",
    "## Results\n",
    "Model Size: ~283M parameters\n",
    "\n",
    "The model learns to generate coherent children's stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZxYsvpLdejO"
   },
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNtDVgKnKyjF",
    "outputId": "0a658997-663b-4ca9-aeb7-41770b340667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lV2XbOdp2Glg",
    "outputId": "a8d961ea-cf1d-4e29-a60e-c7ea0bc57cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.12\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRlTmgHidn6m"
   },
   "source": [
    "# Load TinyStories Dataset\n",
    "\n",
    "This cell loads the TinyStories dataset from Hugging Face:\n",
    "\n",
    "- **Dataset**: `roneneldan/TinyStories` - A collection of simple children's stories\n",
    "- **Size**: ~2.1 million training examples\n",
    "- **Format**: Text-only stories for language model training\n",
    "- **Purpose**: Provides base language understanding for the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "8772c6eadce449dcad63f6948dd7c79b",
      "fa6b599a5c6f4228a24a73189c4dc9d4",
      "e7ed27f9197842ecbae79811401f2020",
      "36f39fb882f14f58a82d892e0f90fe6d",
      "c9584d39b61b43e58e7ae1e9411c5c82",
      "5580726b2a314016ad787353e29fc2fe",
      "5d830d6460cf412692a811380058edfa",
      "2bd7854d90014febaa563f8a64640d7a",
      "8bfc42c57e4d4735aae3e36f5468426e",
      "5272e1c81c7e43c182ec602d3ebbf31e",
      "a270c006bf77451bbfbe8850b458cadc",
      "8fdfac57a4d940028331b285fac30d3a",
      "4ec94403655c4964ae681a1a1ca0d412",
      "9e947cd9704140eabec595eeace93ff3",
      "c63179666fc8460da7b957ba44c80108",
      "2a4a188f4d7e4dec8e5e508dddf17771",
      "1ff9682d30b44decb93eacc5ee480d37",
      "487e9ac3821246c8a111f4fb50a94f0a",
      "8969baf5ec1147e2a93cbc848194cd70",
      "a39849747c674613849020e0e09564b5",
      "eaf38bf2e914472abbb5129d816d2c10",
      "bfb08d4826eb4727abc47854ea625679",
      "0059830111ed454b80f1c2e204064a97",
      "2ba430bc2ce6444eb3362a8fe94c917a",
      "de85e1adf1894d29a3543fbdb2a7fe05",
      "2f54a933ca304fde88a3c9b7be6a3eba",
      "37d03b8803b34388859f88df8b4e8f8c",
      "74baa69623d1423ca52ad6bcd5dc00f7",
      "063139e5ed174bfc9b6f50959e30bfdc",
      "908b5e5378d542b09fdb43a0929af47c",
      "eb00136de35e455ca578e6c86620eed4",
      "228541269cdc4ee8b0c1eeb3dccf7025",
      "af02d906a07a49aebe17dbfd79cf1df8",
      "9f0a1fa650324baaa8708321029a7820",
      "d58b472a5f9f473b8e90d6e8a3c2dee2",
      "4554deea19344edab0011feb39f82b2d",
      "74f68407fa71461e99196de99a74cd27",
      "7f834590b9a64ad5b50298ad16d63e0e",
      "a3a872ad9dea4542909d6870747b29eb",
      "1e9244b8b8b141a2bfdc80c3332c2dc7",
      "5587075bddb74d868fa1c37f0ce57ce6",
      "d9af3acccab14bce85dc01cfc00130c1",
      "940c5820492b4e78a41ee65241fa9b6e",
      "6f4109df16664034be9f1853995a0591",
      "db9b2d729bea4883b9d4518e98abb306",
      "521939ceb936407cbfbd63696ab6331b",
      "4cdb5bbef6cd487491dc10079434bbb1",
      "9de59f65c5af44ccb3f47586dc14b7af",
      "d6c90fdf21a54678af825e055bf5686e",
      "289abf5277604341bcd7ce3186653a4a",
      "ce355c2e62074b00b7e73704766f6599",
      "48132c68169347ae88ea38f321d4e4c1",
      "f33ebc4a7c7a43fa96281853092b3d16",
      "8e919892caa4440293ff7f59a9868c62",
      "395f7d298bca43ae9932e31206fd0674",
      "b5fe1edc5303473eb1fb8fef74fb4ca8",
      "b0497a6f25e342e394330084c34aa1af",
      "4fb3a2f2950e4068a1bb5e1b0abb690a",
      "6ef05305fc5842c6be1f95341b9ce6e7",
      "0a09021efdd14accaa39446acc3cc79a",
      "1e19561b388841a4bfac3504343e6bca",
      "1800ec5c163e4b37b4e03f7db800fe9d",
      "bd8cb352310540f9bbe30ecd93771314",
      "44838332ca1c486fabfc7fbfefa2a55e",
      "209710f2aa3e4dd9951cd98cc5fd8a98",
      "73b137bd883d450ebfeafb6fe18ab424",
      "4d963e39748d4361b4b5d62d1414da9d",
      "e0542b2375e04c1a8328c29895ddba15",
      "bf87caf9d41a4cb59c66e479dac56d5b",
      "6c281345963b48fdb972d3c9bdf61afd",
      "f0ea42ae3c57432d94081f5b06e0b23e",
      "9c5e39b975784344a6032ec00248041e",
      "5913271774ba454099cc2cde2dd579d3",
      "1ff25262aa374fb7ba5ad34fc108e82a",
      "2c1f8a98ae6d4f9c8efabdf7b0f765dd",
      "2638f4e4fbcd4b8e94de445fa4433ca1",
      "91682b1a3e5f4fb881264271b0482805",
      "99653c42a0d549fa8d5fa92d20396603",
      "1c32623201a746cbac103958008f6fe1",
      "5f9cdef30c374d2ead45f0692f88ff55",
      "cc3b4b5e91614cfb825c06b43676092b",
      "fb583a8110bf415889c5d80c0ee7b6dc",
      "b5c86a6c4b7e4387a2b6e30478ea516f",
      "fcc4d97f8f344ef6be1555734448fbda",
      "58c05b9c91bd4a3a9aa19e7d56958ad1",
      "1ef04eb5317f413c9f1f7be3236a7d0a",
      "8482aea8db2c401c85c731c9f7ee73c4",
      "b5ad8a74df1f4e1fadb0c9c8e3b015b1"
     ]
    },
    "id": "RvUABYMtK9dp",
    "outputId": "f541a0f3-80c9-4c21-e638-be6e899cca4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vis042O0d2A_"
   },
   "source": [
    "# Dataset Tokenization & Preprocessing\n",
    "\n",
    "This cell handles the complete tokenization pipeline:\n",
    "\n",
    "## **What it does:**\n",
    "- **Installs tiktoken**: Alternative tokenizer library\n",
    "- **Loads Qwen3 tokenizer**: Latest tokenizer with Qwen2.5 fallback\n",
    "- **Tokenizes dataset**: Converts text to token IDs for training\n",
    "- **Creates binary files**: Saves tokenized data as `.bin` files `train.bin & validation.bin` for fast loading\n",
    "\n",
    "## **Process:**\n",
    "1. **Tokenization**: Converts 2.1M text examples to token sequences\n",
    "2. **Memory mapping**: Creates efficient binary storage format\n",
    "3. **Batch processing**: Processes data in 1024 batches for memory efficiency\n",
    "4. **Progress tracking**: Shows real-time tokenization progress\n",
    "\n",
    "## **Output:**\n",
    "- `train.bin`: Tokenized training data\n",
    "- `validation.bin`: Tokenized validation data\n",
    "- **Time**: ~20 minutes for complete tokenization\n",
    "- **Size**: Optimized binary format for fast training\n",
    "\n",
    "## **Purpose:**\n",
    "Prepares the dataset in the optimal format for efficient model training with minimal memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394,
     "referenced_widgets": [
      "e36d429829b14493b23e165825ea04c8",
      "1ee66cc8788d498f9dad663bc372b6d2",
      "7040305d1aac41d0a19b6eea2c590105",
      "4694d7b338184efaa2e991e54259b752",
      "609cfff1c8cd42acb5b2fb2fa7bb23c9",
      "07768d7c354440e6b65341485cc3b2bd",
      "d68ea9468d5a4457b050e61eda65ac80",
      "70d2a3bd581c4a6a9a3c40e1900fb79a",
      "74ae9c1bb7ea45b394dd23fed5db3e20",
      "ee1761c654c94c20b00ce0c49b111d9a",
      "c78d38f56cf64432877cf8f2f3a556c4",
      "13aac3cacfdd46ee874611ffca346bfb",
      "c979702ba2ab45b68369a4387bf71a66",
      "f2cf84d30c374c85b038a0f3313e718c",
      "3786805dd5f149a598eadc84c49fe71e",
      "fd05e724a6a44b27852e2daa6062c551",
      "5bd810507a5448f6a0633ad636c15d01",
      "3cac53c951474d86803bdec3a02e16ce",
      "d0580ab23095428daecd85ac37e965dd",
      "c2b113193a284606ab94110245825c20",
      "19292b776a794462b6ad2c47c11032ba",
      "df163b90363840cc824d17ec836496b8",
      "63d6777130b74aab831a354bbfa7e64b",
      "6b0774034a7240ca84842d38884c48ce",
      "d1476a3dfe914cfb947b75738c88689c",
      "2be2f662a60a4fdfa04ab1060d06e90e",
      "69af2655e78f46748b691bbb01c0b2e3",
      "0f211b81030e4b83a01f8754c01727a6",
      "5d4b41ec3c734b36a69a1d9fcf72b188",
      "7fe1b6f47e474d2ea59112ff2ca69662",
      "d2fb3a7d8c1f45be971c01421b2e0890",
      "b648a790cd96465f8d4337d7299eaefc",
      "548ce9b49e6540c2a1d48cc60d5066a4",
      "26f00aeab3c0457995f999e70ec4bee9",
      "899b2100f21a4957be07cccbd18d8f00",
      "229033c2bd934fa1b2c8b8ace0ada947",
      "5996a4e7447946fd9529167205944421",
      "752a2009b8c7409a940a8f4a10cc91b4",
      "c1e063cdf4fc4045a4b1df8370dd58c8",
      "224e9f7676b94931836d53d175a496be",
      "364ba2d6036f4e25afd290ca4fc5ffd5",
      "bec3a3e0b358482f96f7f7a7364ec974",
      "fdd7010ae1c84b79a6357a6384d7a2b8",
      "0a47b5254fdd4a4da9c6bcbb48903b5b",
      "730ac8759e784cae8deef8138b73981b",
      "f4ce74f52cd04ad1ac494565f5a8fa45",
      "8898053defe6474cad48abfbf5535eb2",
      "3a597c1fa27048f09428dddb512bf1b3",
      "2c58a20e619f48a1a88d776fb78f9c42",
      "817cf96d65cd491fa4437050155e382b",
      "26803e07be7a482584feceb48112f203",
      "9ae96c7898794a789df2668b8787738f",
      "e120faf0ff004b4f80984310d13d9a88",
      "87e7b80e779449e287f0c272e2407680",
      "0090326e06004eb5b416578defc0ebbe",
      "304e60a83b0949d5a0454bbb82ae088a",
      "cec88e4c00c24ed49f2728193efb325a",
      "a24c4c43443d43c0baa9be86633fb64b",
      "8d310ff7c3994c1c86106e4775e6b28e",
      "34ab1f0c3f744f80bb35b6c8692511f8",
      "f63174ab39cf4f578ef916e5f2f0180c",
      "b3a3f5c8606d46c18bee1ef4bf751702",
      "15c31f3875594b79a578eadde88f5651",
      "b518495028d94e8f9cef0ef3748539c6",
      "7da64dca3793497dbe2626ee1f40dfe3",
      "c5227f0d820f47bab7bb4cee50ccbffb",
      "52b775de90cb4a9a8689cec2aeadcc67",
      "ef264749f63f4bc987573eac2da3cf8c",
      "a2049c6bc75040998d9f3025957d531d",
      "d8410eb09e534d36bddac93b51f75177",
      "d67092fa962b4fc7810ed51111d3ceda",
      "7a3b92b74f394cb599a7abd4f1c2ca0d",
      "fabdec667cd44f11ae3db7fd085269ff",
      "7a6fc14c00d94985acc293ff11de2803",
      "20ee4ba371404941811bddacbdf5135a",
      "5c2474a6106841a99f0b19540485dd04",
      "1bd5e0ef6d62450a81caff062ece26ab",
      "5c6286675d9d4369beefe1d86c5775f0",
      "41d91faa57e44fa08316f972f519318b",
      "a0f576c680224170887278ba925bb5e6",
      "d2cb373b916542e58b638684d8599980",
      "01983e698d9d4275a1a762bbc4581785",
      "0d9854191f2e4679ad5aad4932d06c91",
      "afc126b794c741fc9cfba259ca906ee0",
      "e5e11b87c86c472a9aca7bd6e97a6b73",
      "f673c116bed84f37977f7897bc184289",
      "b1d2ccc675454923993019fc8e9617ad",
      "bc78c0e1bcd64483929ddb032766135f"
     ]
    },
    "id": "h8KkO7ZSK_19",
    "outputId": "72f59fa2-62e5-4d13-ca61-650911ff3b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "\n",
    "\n",
    "def process(example):\n",
    "    ids = tokenizer.encode(example['text'], add_special_tokens=False)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8,\n",
    "        )\n",
    "\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint32 if tokenizer.vocab_size > 65536 else np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjZr84oteXNT"
   },
   "source": [
    "# Batch Data Loading Function\n",
    "\n",
    "This function efficiently loads training batches from the tokenized dataset:\n",
    "\n",
    "## **What it does:**\n",
    "- **Memory mapping**: Uses `np.memmap` for efficient data loading without loading entire dataset into RAM\n",
    "- **Random sampling**: Selects random sequences from the dataset for training\n",
    "- **Sequence generation**: Creates input (x) and target (y) sequences for language modeling\n",
    "- **GPU optimization**: Uses `pin_memory()` and `non_blocking=True` for faster GPU transfer\n",
    "\n",
    "## **Process:**\n",
    "1. **Load data**: Opens binary file with memory mapping\n",
    "2. **Random indices**: Selects random starting positions\n",
    "3. **Create sequences**: Input sequence (x) and target sequence (y) shifted by 1 token\n",
    "4. **GPU transfer**: Efficiently moves data to GPU with memory pinning\n",
    "\n",
    "## **Parameters:**\n",
    "- **batch_size**: Number of sequences per batch\n",
    "- **block_size**: Length of each sequence (context window)\n",
    "- **split**: 'train' or 'validation' dataset\n",
    "\n",
    "## **Purpose:**\n",
    "Provides efficient, memory-optimized batch loading for training with minimal memory overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjBjynd5NLmJ"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint32, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint32, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyK_vjtUemu_"
   },
   "source": [
    "# RoPE (Rotary Position Embedding) Implementation\n",
    "\n",
    "This section implements the RoPE mechanism used in modern transformer models:\n",
    "\n",
    "## **What it does:**\n",
    "- **Position encoding**: Encodes positional information directly into attention queries and keys\n",
    "- **Rotation-based**: Uses rotation matrices to encode position information\n",
    "- **Frequency-based**: Different frequencies for different dimensions\n",
    "- **Efficient**: No additional parameters, just mathematical transformations\n",
    "\n",
    "## **Functions:**\n",
    "1. **`compute_rope_params`**: Pre-computes cosine and sine values for all positions\n",
    "2. **`apply_rope`**: Applies rotation to attention queries and keys\n",
    "\n",
    "## **Benefits:**\n",
    "- **Better long sequences**: Handles longer context than traditional positional encodings\n",
    "- **Relative positions**: Model can understand relative distances between tokens\n",
    "- **Efficient**: No learnable parameters, just mathematical operations\n",
    "- **Modern standard**: Used in GPT, LLaMA, Qwen, and other state-of-the-art models\n",
    "\n",
    "## **Purpose:**\n",
    "Provides the positional encoding mechanism that allows the model to understand token positions in sequences, essential for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lOFj1SaOJif"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
    "    positions = torch.arange(context_length, dtype=dtype)\n",
    "    angles = positions[:, None] * inv_freq[None, :]\n",
    "    angles = torch.cat([angles, angles], dim=1)\n",
    "\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def apply_rope(x, cos, sin):\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    x1 = x[..., : head_dim // 2]\n",
    "    x2 = x[..., head_dim // 2 :]\n",
    "\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2UeCNTwe4Q3"
   },
   "source": [
    "# Qwen3 Model Architecture Implementation\n",
    "\n",
    "This section implements the complete Qwen3 transformer architecture with modern optimizations:\n",
    "\n",
    "## **Core Components:**\n",
    "\n",
    "### **RMSNorm**\n",
    "- **Root Mean Square Normalization**: More stable than LayerNorm\n",
    "- **Efficient**: No mean centering, just variance normalization\n",
    "- **Modern standard**: Used in LLaMA, Qwen, and other state-of-the-art models\n",
    "\n",
    "### **Grouped Query Attention (GQA)**\n",
    "- **Memory efficient**: Reduces memory usage by sharing key-value pairs\n",
    "- **Performance**: Maintains quality while reducing computational cost\n",
    "- **QK Normalization**: Additional normalization for better training stability\n",
    "\n",
    "### **SwiGLU FeedForward**\n",
    "- **Advanced activation**: Swish activation with gated linear units\n",
    "- **Better performance**: Superior to standard ReLU-based feedforward\n",
    "- **Modern architecture**: Used in PaLM, LLaMA, and other large models\n",
    "\n",
    "### **Transformer Block**\n",
    "- **Pre-norm**: Normalization before attention and feedforward\n",
    "- **Residual connections**: Skip connections for gradient flow\n",
    "- **Complete block**: Attention + FeedForward with proper normalization\n",
    "\n",
    "## **Complete Model**\n",
    "- **Embedding layer**: Token to vector conversion\n",
    "- **Multiple transformer blocks**: Stacked for deep learning\n",
    "- **Output head**: Final layer for vocabulary prediction\n",
    "- **RoPE integration**: Positional encoding built into attention\n",
    "- **Generation capability**: Built-in text generation with temperature and top-k sampling\n",
    "\n",
    "## **Purpose:**\n",
    "Implements a state-of-the-art transformer architecture optimized for efficient training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9WyygMfOjuF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Qwen3 RMSNorm implementation\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x_f = x.float()\n",
    "        var = x_f.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = x_f * torch.rsqrt(var + self.eps)\n",
    "        return (x_norm * self.weight.float()).to(input_dtype)\n",
    "\n",
    "# Qwen3 Grouped Query Attention\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_in, num_heads, num_kv_groups, head_dim=None, dtype=None):\n",
    "        super().__init__()\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        if head_dim is None:\n",
    "            head_dim = d_in // num_heads\n",
    "\n",
    "        self.head_dim = head_dim\n",
    "        self.d_out = num_heads * head_dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
    "\n",
    "        # Qwen3 uses QK normalization\n",
    "        self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "        self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
    "\n",
    "        self.scaling = head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # QK normalization (Qwen3 feature)\n",
    "        queries = self.q_norm(queries)\n",
    "        keys = self.k_norm(keys)\n",
    "\n",
    "        # Apply RoPE\n",
    "        queries = apply_rope(queries, cos, sin)\n",
    "        keys = apply_rope(keys, cos, sin)\n",
    "\n",
    "        # Expand K and V for GQA\n",
    "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
    "        values = values.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        queries = queries * self.scaling\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context)\n",
    "\n",
    "# Qwen3 SwiGLU FeedForward\n",
    "class SwiGLUFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dtype=None):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False, dtype=dtype)\n",
    "        self.up_proj = nn.Linear(d_model, d_ff, bias=False, dtype=dtype)\n",
    "        self.down_proj = nn.Linear(d_ff, d_model, bias=False, dtype=dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # SwiGLU: Swish(gate) * up\n",
    "        gate = F.silu(self.gate_proj(x))\n",
    "        up = self.up_proj(x)\n",
    "        return self.down_proj(gate * up)\n",
    "\n",
    "# Qwen3 Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_kv_groups, d_ff, dtype=None):\n",
    "        super().__init__()\n",
    "        self.attention = GroupedQueryAttention(d_model, n_heads, n_kv_groups, dtype=dtype)\n",
    "        self.feed_forward = SwiGLUFeedForward(d_model, d_ff, dtype=dtype)\n",
    "        self.norm1 = RMSNorm(d_model, eps=1e-6)\n",
    "        self.norm2 = RMSNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        # Pre-norm attention\n",
    "        attn_out = self.attention(self.norm1(x), mask, cos, sin)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # Pre-norm feedforward\n",
    "        ff_out = self.feed_forward(self.norm2(x))\n",
    "        x = x + ff_out\n",
    "\n",
    "        return x\n",
    "\n",
    "# Qwen3 Model\n",
    "class Qwen3Model(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_kv_groups, n_layers, d_ff, max_seq_len, dtype=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model, dtype=dtype)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, n_kv_groups, d_ff, dtype=dtype)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.final_norm = RMSNorm(d_model, eps=1e-6)\n",
    "        self.out_head = nn.Linear(d_model, vocab_size, bias=False, dtype=dtype)\n",
    "\n",
    "        # RoPE parameters\n",
    "        head_dim = d_model // n_heads\n",
    "        cos, sin = compute_rope_params(\n",
    "            head_dim=head_dim,\n",
    "            context_length=max_seq_len,\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def _create_causal_mask(self, seq_len, device):\n",
    "        return torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "    def forward(self, input_ids, targets=None):\n",
    "        b, seq_len = input_ids.shape\n",
    "        x = self.tok_emb(input_ids) * (self.d_model ** 0.5)\n",
    "        mask = self._create_causal_mask(seq_len, x.device)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask, self.cos, self.sin)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.max_seq_len else idx[:, -self.max_seq_len:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float(\"-inf\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK5kLDrtfC7l"
   },
   "source": [
    "# Model Evaluation Function\n",
    "\n",
    "This function evaluates the model's performance on both training and validation sets:\n",
    "\n",
    "## **What it does:**\n",
    "- **Loss estimation**: Computes average loss over multiple batches\n",
    "- **Dual evaluation**: Tests on both training and validation data\n",
    "- **Model state management**: Switches to eval mode, then back to train mode\n",
    "- **No gradients**: Uses `torch.no_grad()` for memory efficiency\n",
    "\n",
    "## **Process:**\n",
    "1. **Set eval mode**: Disables dropout and batch norm updates\n",
    "2. **Sample batches**: Gets random batches from train/val splits\n",
    "3. **Compute losses**: Calculates loss for each batch\n",
    "4. **Average results**: Returns mean loss across all samples\n",
    "5. **Restore train mode**: Re-enables training components\n",
    "\n",
    "## **Parameters:**\n",
    "- **eval_iters**: Number of batches to evaluate (default: 200)\n",
    "- **Returns**: Dictionary with 'train' and 'val' loss values\n",
    "\n",
    "## **Purpose:**\n",
    "Provides unbiased loss estimates for monitoring training progress and detecting overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb8-iXazOlAn"
   },
   "outputs": [],
   "source": [
    "def estimate_loss(model, eval_iters=200):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMAxOLK7CFYE",
    "outputId": "6fa5ffd3-6068-4c2c-f04e-88b49d95769b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-SXM4-40GB\n",
      "42.474471424 GB\n",
      "bf16 supported: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(\"bf16 supported:\", torch.cuda.is_bf16_supported())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNXAaV00fQr6"
   },
   "source": [
    "# Model Configuration & Initialization\n",
    "\n",
    "This section defines the model architecture and creates the Qwen3 model instance:\n",
    "\n",
    "## **Configuration Parameters:**\n",
    "- **vocab_size**: Automatically uses tokenizer's vocabulary size\n",
    "- **d_model**: 768 - Model embedding dimension\n",
    "- **n_heads**: 12 - Number of attention heads\n",
    "- **n_kv_groups**: 6 - Key-value groups for Grouped Query Attention\n",
    "- **n_layers**: 12 - Number of transformer layers\n",
    "- **d_ff**: 3072 - Feed-forward dimension (4x d_model)\n",
    "- **max_seq_len**: Uses defined block_size for context length\n",
    "- **dtype**: Uses determined precision (bfloat16/float16)\n",
    "\n",
    "## **Model Creation:**\n",
    "- **Random seed**: Set to 123 for reproducible results\n",
    "- **Model instantiation**: Creates Qwen3Model with specified configuration\n",
    "- **Parameter count**: ~283M parameters (efficient size)\n",
    "\n",
    "## **Purpose:**\n",
    "Initializes the complete model architecture with optimized parameters for efficient training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaaf707c"
   },
   "outputs": [],
   "source": [
    "\n",
    "block_size = 2048\n",
    "ptdtype = torch.bfloat16\n",
    "\n",
    "# Define the model configuration (using reasonable defaults for Qwen3)\n",
    "QWEN3_CONFIG = {\n",
    "    'vocab_size': tokenizer.vocab_size,  # Use the tokenizer's vocab size\n",
    "    'd_model': 768,  # Example dimension\n",
    "    'n_heads': 12,   # Example number of heads\n",
    "    'n_kv_groups': 6, # Example number of KV groups for GQA (must divide n_heads)\n",
    "    'n_layers': 12,  # Example number of layers\n",
    "    'd_ff': 3072,  # Example feed-forward dimension (often 4*d_model)\n",
    "    'max_seq_len': block_size, # Use the defined block_size\n",
    "    'dtype': ptdtype, # Use the determined dtype\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "# Instantiate the model\n",
    "model = Qwen3Model(**QWEN3_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egTpANNkfcWc"
   },
   "source": [
    "# Training Configuration & Setup\n",
    "\n",
    "This section configures all training parameters and optimizers:\n",
    "\n",
    "## **Training Parameters:**\n",
    "- **learning_rate**: 1e-4 - Initial learning rate\n",
    "- **max_iters**: 150,000 - Total training iterations\n",
    "- **warmup_steps**: 1,000 - Linear warmup period\n",
    "- **min_lr**: 5e-4 - Minimum learning rate for cosine decay\n",
    "- **eval_iters**: 500 - Evaluation frequency\n",
    "- **batch_size**: 32 - Training batch size\n",
    "- **block_size**: 128 - Context window length\n",
    "- **gradient_accumulation_steps**: 32 - Effective batch size = 32 × 32 = 1024\n",
    "\n",
    "## **Device & Precision:**\n",
    "- **Auto-detect GPU**: Uses CUDA if available, falls back to CPU\n",
    "- **Mixed precision**: bfloat16 (preferred) or float16 for memory efficiency\n",
    "- **Autocast context**: Enables automatic mixed precision training\n",
    "\n",
    "## **Optimizer & Scheduler:**\n",
    "- **AdamW optimizer**: With weight decay for regularization\n",
    "- **Learning rate schedule**: Linear warmup → Cosine decay\n",
    "- **Gradient scaling**: For float16 stability\n",
    "\n",
    "## **Purpose:**\n",
    "Sets up optimized training configuration for efficient model training with mixed precision and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "410ace5a"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "max_iters = 150000\n",
    "warmup_steps = 1000\n",
    "min_lr = 5e-4\n",
    "eval_iters = 500\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "from contextlib import nullcontext\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9)\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters=warmup_steps)\n",
    "scheduler_decay = CosineAnnealingLR(optimizer, T_max=max_iters - warmup_steps, eta_min=min_lr)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps])\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5DWptVufomS"
   },
   "source": [
    "# Model Training Loop\n",
    "\n",
    "This section runs the complete training process with monitoring and checkpointing:\n",
    "\n",
    "## **Training Process:**\n",
    "- **Iterative training**: 150,000 iterations with progress tracking\n",
    "- **Periodic evaluation**: Every 500 iterations on train/val sets\n",
    "- **Loss monitoring**: Tracks and displays training progress\n",
    "- **Best model saving**: Automatically saves model with lowest validation loss\n",
    "- **Learning rate tracking**: Shows current learning rate during training\n",
    "\n",
    "## **Training Features:**\n",
    "- **Gradient accumulation**: Effective batch size of 1024 (32 × 32)\n",
    "- **Mixed precision**: Uses autocast for memory efficiency\n",
    "- **Gradient clipping**: Prevents exploding gradients (max_norm=0.5)\n",
    "- **Learning rate scheduling**: Automatic warmup and decay\n",
    "- **Progress tracking**: Real-time progress bar and loss monitoring\n",
    "\n",
    "## **Output:**\n",
    "- **Real-time updates**: Loss values and learning rate every 500 steps\n",
    "- **Model checkpoint**: `qwen3_slm.pt` - best model based on validation loss\n",
    "- **Loss history**: Lists for plotting training curves\n",
    "\n",
    "## **Purpose:**\n",
    "Trains the model with modern optimization techniques and automatically saves the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "31b5f9cc7cc4444f853f7922e8ab38c6",
      "a2c1f202c1eb4cd1bd3081517f063a8d",
      "7b677dd4f56c4deeb01b225dccc27720",
      "ab32ae51760548ba8fa6831d749a0172",
      "67a2c459197947fc8545c4724008b65e",
      "c228562194f54e3284eca9c554444c59",
      "458d304b9a5c4368bef02a98cca4bda5",
      "cbbd4fb3ff184cc29f2526fc23bb79fd",
      "7d7ffc787ae24cb28206fb517592098b",
      "dbd3f29f213246edbae360040d4f9786",
      "fcb72c23dc5541c3a99a1c6a22760294"
     ]
    },
    "id": "dFnwFYAORo5c",
    "outputId": "6103fc31-dcf8-4c0d-fa70-d998bf3e4724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: train loss 11.8512, val loss 11.8497\n",
      "The current learning rate: 0.00007\n",
      "Epoch 1000: train loss 10.8687, val loss 10.8653\n",
      "The current learning rate: 0.00010\n",
      "Epoch 1500: train loss 8.5125, val loss 8.4963\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2000: train loss 6.8183, val loss 6.8175\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2500: train loss 5.8612, val loss 5.8478\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3000: train loss 5.3758, val loss 5.3655\n",
      "The current learning rate: 0.00010\n",
      "Epoch 3500: train loss 5.0508, val loss 5.0295\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4000: train loss 4.7923, val loss 4.7842\n",
      "The current learning rate: 0.00010\n",
      "Epoch 4500: train loss 4.6086, val loss 4.5772\n",
      "The current learning rate: 0.00010\n",
      "Epoch 5000: train loss 4.4366, val loss 4.4447\n",
      "The current learning rate: 0.00010\n",
      "Epoch 5500: train loss 4.3342, val loss 4.3244\n",
      "The current learning rate: 0.00010\n",
      "Epoch 6000: train loss 4.2323, val loss 4.2225\n",
      "The current learning rate: 0.00010\n",
      "Epoch 6500: train loss 4.1524, val loss 4.1330\n",
      "The current learning rate: 0.00010\n",
      "Epoch 7000: train loss 4.0945, val loss 4.0859\n",
      "The current learning rate: 0.00010\n",
      "Epoch 7500: train loss 4.0327, val loss 4.0193\n",
      "The current learning rate: 0.00010\n",
      "Epoch 8000: train loss 3.9680, val loss 3.9674\n",
      "The current learning rate: 0.00010\n",
      "Epoch 8500: train loss 3.9098, val loss 3.9194\n",
      "The current learning rate: 0.00010\n",
      "Epoch 9000: train loss 3.8706, val loss 3.8730\n",
      "The current learning rate: 0.00010\n",
      "Epoch 9500: train loss 3.8290, val loss 3.8159\n",
      "The current learning rate: 0.00010\n",
      "Epoch 10000: train loss 3.7777, val loss 3.7746\n",
      "The current learning rate: 0.00010\n",
      "Epoch 10500: train loss 3.7295, val loss 3.7488\n",
      "The current learning rate: 0.00010\n",
      "Epoch 11000: train loss 3.7178, val loss 3.7082\n",
      "The current learning rate: 0.00010\n",
      "Epoch 11500: train loss 3.6816, val loss 3.6627\n",
      "The current learning rate: 0.00010\n",
      "Epoch 12000: train loss 3.6501, val loss 3.6480\n",
      "The current learning rate: 0.00011\n",
      "Epoch 12500: train loss 3.6239, val loss 3.6284\n",
      "The current learning rate: 0.00011\n",
      "Epoch 13000: train loss 3.5935, val loss 3.5895\n",
      "The current learning rate: 0.00011\n",
      "Epoch 13500: train loss 3.5833, val loss 3.5729\n",
      "The current learning rate: 0.00011\n",
      "Epoch 14000: train loss 3.5591, val loss 3.5594\n",
      "The current learning rate: 0.00011\n",
      "Epoch 14500: train loss 3.5345, val loss 3.5345\n",
      "The current learning rate: 0.00011\n",
      "Epoch 15000: train loss 3.5038, val loss 3.5065\n",
      "The current learning rate: 0.00011\n",
      "Epoch 15500: train loss 3.5009, val loss 3.4951\n",
      "The current learning rate: 0.00011\n",
      "Epoch 16000: train loss 3.4859, val loss 3.4830\n",
      "The current learning rate: 0.00011\n",
      "Epoch 16500: train loss 3.4584, val loss 3.4635\n",
      "The current learning rate: 0.00011\n",
      "Epoch 17000: train loss 3.4461, val loss 3.4482\n",
      "The current learning rate: 0.00011\n",
      "Epoch 17500: train loss 3.4254, val loss 3.4368\n",
      "The current learning rate: 0.00011\n",
      "Epoch 18000: train loss 3.4268, val loss 3.4238\n",
      "The current learning rate: 0.00011\n",
      "Epoch 18500: train loss 3.4009, val loss 3.4087\n",
      "The current learning rate: 0.00011\n",
      "Epoch 19000: train loss 3.3919, val loss 3.4042\n",
      "The current learning rate: 0.00011\n",
      "Epoch 19500: train loss 3.3828, val loss 3.3802\n",
      "The current learning rate: 0.00012\n",
      "Epoch 20000: train loss 3.3595, val loss 3.3662\n",
      "The current learning rate: 0.00012\n",
      "Epoch 20500: train loss 3.3532, val loss 3.3519\n",
      "The current learning rate: 0.00012\n",
      "Epoch 21000: train loss 3.3361, val loss 3.3491\n",
      "The current learning rate: 0.00012\n",
      "Epoch 21500: train loss 3.3276, val loss 3.3202\n",
      "The current learning rate: 0.00012\n",
      "Epoch 22000: train loss 3.3247, val loss 3.3128\n",
      "The current learning rate: 0.00012\n",
      "Epoch 22500: train loss 3.3059, val loss 3.3096\n",
      "The current learning rate: 0.00012\n",
      "Epoch 23000: train loss 3.3077, val loss 3.3105\n",
      "The current learning rate: 0.00012\n",
      "Epoch 23500: train loss 3.2941, val loss 3.2870\n",
      "The current learning rate: 0.00012\n",
      "Epoch 24000: train loss 3.2812, val loss 3.2786\n",
      "The current learning rate: 0.00012\n",
      "Epoch 24500: train loss 3.2684, val loss 3.2676\n",
      "The current learning rate: 0.00012\n",
      "Epoch 25000: train loss 3.2630, val loss 3.2580\n",
      "The current learning rate: 0.00013\n",
      "Epoch 25500: train loss 3.2522, val loss 3.2506\n",
      "The current learning rate: 0.00013\n",
      "Epoch 26000: train loss 3.2452, val loss 3.2336\n",
      "The current learning rate: 0.00013\n",
      "Epoch 26500: train loss 3.2384, val loss 3.2402\n",
      "The current learning rate: 0.00013\n",
      "Epoch 27000: train loss 3.2455, val loss 3.2484\n",
      "The current learning rate: 0.00013\n",
      "Epoch 27500: train loss 3.2325, val loss 3.2288\n",
      "The current learning rate: 0.00013\n",
      "Epoch 28000: train loss 3.2204, val loss 3.2205\n",
      "The current learning rate: 0.00013\n",
      "Epoch 28500: train loss 3.2083, val loss 3.2095\n",
      "The current learning rate: 0.00013\n",
      "Epoch 29000: train loss 3.1972, val loss 3.1934\n",
      "The current learning rate: 0.00013\n",
      "Epoch 29500: train loss 3.1916, val loss 3.1952\n",
      "The current learning rate: 0.00014\n",
      "Epoch 30000: train loss 3.1947, val loss 3.1855\n",
      "The current learning rate: 0.00014\n",
      "Epoch 30500: train loss 3.1830, val loss 3.1926\n",
      "The current learning rate: 0.00014\n",
      "Epoch 31000: train loss 3.1752, val loss 3.1631\n",
      "The current learning rate: 0.00014\n",
      "Epoch 31500: train loss 3.1745, val loss 3.1712\n",
      "The current learning rate: 0.00014\n",
      "Epoch 32000: train loss 3.1587, val loss 3.1648\n",
      "The current learning rate: 0.00014\n",
      "Epoch 32500: train loss 3.1548, val loss 3.1584\n",
      "The current learning rate: 0.00014\n",
      "Epoch 33000: train loss 3.1516, val loss 3.1522\n",
      "The current learning rate: 0.00014\n",
      "Epoch 33500: train loss 3.1537, val loss 3.1445\n",
      "The current learning rate: 0.00015\n",
      "Epoch 34000: train loss 3.1406, val loss 3.1452\n",
      "The current learning rate: 0.00015\n",
      "Epoch 34500: train loss 3.1327, val loss 3.1294\n",
      "The current learning rate: 0.00015\n",
      "Epoch 35000: train loss 3.1255, val loss 3.1339\n",
      "The current learning rate: 0.00015\n",
      "Epoch 35500: train loss 3.1187, val loss 3.1178\n",
      "The current learning rate: 0.00015\n",
      "Epoch 36000: train loss 3.1187, val loss 3.1125\n",
      "The current learning rate: 0.00015\n",
      "Epoch 36500: train loss 3.1179, val loss 3.1137\n",
      "The current learning rate: 0.00015\n",
      "Epoch 37000: train loss 3.0934, val loss 3.0924\n",
      "The current learning rate: 0.00015\n",
      "Epoch 37500: train loss 3.1088, val loss 3.1065\n",
      "The current learning rate: 0.00016\n",
      "Epoch 38000: train loss 3.0747, val loss 3.0810\n",
      "The current learning rate: 0.00016\n",
      "Epoch 38500: train loss 3.0902, val loss 3.0873\n",
      "The current learning rate: 0.00016\n",
      "Epoch 39000: train loss 3.0867, val loss 3.0876\n",
      "The current learning rate: 0.00016\n",
      "Epoch 39500: train loss 3.0755, val loss 3.0833\n",
      "The current learning rate: 0.00016\n",
      "Epoch 40000: train loss 3.0705, val loss 3.0788\n",
      "The current learning rate: 0.00016\n",
      "Epoch 40500: train loss 3.1412, val loss 3.1399\n",
      "The current learning rate: 0.00017\n",
      "Epoch 41000: train loss 3.0638, val loss 3.0677\n",
      "The current learning rate: 0.00017\n",
      "Epoch 41500: train loss 3.0537, val loss 3.0597\n",
      "The current learning rate: 0.00017\n",
      "Epoch 42000: train loss 3.0469, val loss 3.0451\n",
      "The current learning rate: 0.00017\n",
      "Epoch 42500: train loss 3.0451, val loss 3.0389\n",
      "The current learning rate: 0.00017\n",
      "Epoch 43000: train loss 3.0323, val loss 3.0395\n",
      "The current learning rate: 0.00017\n",
      "Epoch 43500: train loss 3.0374, val loss 3.0384\n",
      "The current learning rate: 0.00018\n",
      "Epoch 44000: train loss 3.0374, val loss 3.0395\n",
      "The current learning rate: 0.00018\n",
      "Epoch 44500: train loss 3.0243, val loss 3.0430\n",
      "The current learning rate: 0.00018\n",
      "Epoch 45000: train loss 3.0333, val loss 3.0301\n",
      "The current learning rate: 0.00018\n",
      "Epoch 45500: train loss 3.0161, val loss 3.0157\n",
      "The current learning rate: 0.00018\n",
      "Epoch 46000: train loss 3.0105, val loss 3.0062\n",
      "The current learning rate: 0.00018\n",
      "Epoch 46500: train loss 3.0105, val loss 3.0036\n",
      "The current learning rate: 0.00019\n",
      "Epoch 47000: train loss 3.0080, val loss 2.9888\n",
      "The current learning rate: 0.00019\n",
      "Epoch 47500: train loss 2.9880, val loss 2.9955\n",
      "The current learning rate: 0.00019\n",
      "Epoch 48000: train loss 2.9806, val loss 2.9845\n",
      "The current learning rate: 0.00019\n",
      "Epoch 48500: train loss 2.9780, val loss 2.9738\n",
      "The current learning rate: 0.00019\n",
      "Epoch 49000: train loss 2.9710, val loss 2.9634\n",
      "The current learning rate: 0.00019\n",
      "Epoch 49500: train loss 2.9998, val loss 2.9970\n",
      "The current learning rate: 0.00020\n",
      "Epoch 50000: train loss 2.9662, val loss 2.9705\n",
      "The current learning rate: 0.00020\n",
      "Epoch 50500: train loss 2.9756, val loss 2.9816\n",
      "The current learning rate: 0.00020\n",
      "Epoch 51000: train loss 2.9568, val loss 2.9578\n",
      "The current learning rate: 0.00020\n",
      "Epoch 51500: train loss 2.9630, val loss 2.9662\n",
      "The current learning rate: 0.00020\n",
      "Epoch 52000: train loss 2.9448, val loss 2.9501\n",
      "The current learning rate: 0.00020\n",
      "Epoch 52500: train loss 2.9451, val loss 2.9425\n",
      "The current learning rate: 0.00021\n",
      "Epoch 53000: train loss 2.9385, val loss 2.9466\n",
      "The current learning rate: 0.00021\n",
      "Epoch 53500: train loss 2.9403, val loss 2.9391\n",
      "The current learning rate: 0.00021\n",
      "Epoch 54000: train loss 2.9254, val loss 2.9320\n",
      "The current learning rate: 0.00021\n",
      "Epoch 54500: train loss 2.9316, val loss 2.9309\n",
      "The current learning rate: 0.00021\n",
      "Epoch 55000: train loss 2.9268, val loss 2.9285\n",
      "The current learning rate: 0.00022\n",
      "Epoch 55500: train loss 2.9211, val loss 2.9283\n",
      "The current learning rate: 0.00022\n",
      "Epoch 56000: train loss 2.9274, val loss 2.9284\n",
      "The current learning rate: 0.00022\n",
      "Epoch 56500: train loss 2.9112, val loss 2.8954\n",
      "The current learning rate: 0.00022\n",
      "Epoch 57000: train loss 2.8963, val loss 2.8977\n",
      "The current learning rate: 0.00022\n",
      "Epoch 57500: train loss 2.8963, val loss 2.8835\n",
      "The current learning rate: 0.00023\n",
      "Epoch 58000: train loss 2.8905, val loss 2.8873\n",
      "The current learning rate: 0.00023\n",
      "Epoch 58500: train loss 2.8712, val loss 2.8701\n",
      "The current learning rate: 0.00023\n",
      "Epoch 59000: train loss 2.8827, val loss 2.8807\n",
      "The current learning rate: 0.00023\n",
      "Epoch 59500: train loss 2.8639, val loss 2.8807\n",
      "The current learning rate: 0.00023\n",
      "Epoch 60000: train loss 2.8657, val loss 2.8727\n",
      "The current learning rate: 0.00024\n",
      "Epoch 60500: train loss 2.8533, val loss 2.8490\n",
      "The current learning rate: 0.00024\n",
      "Epoch 61000: train loss 2.8566, val loss 2.8536\n",
      "The current learning rate: 0.00024\n",
      "Epoch 61500: train loss 2.8852, val loss 2.8892\n",
      "The current learning rate: 0.00024\n",
      "Epoch 62000: train loss 2.8709, val loss 2.8653\n",
      "The current learning rate: 0.00024\n",
      "Epoch 62500: train loss 2.8444, val loss 2.8391\n",
      "The current learning rate: 0.00025\n",
      "Epoch 63000: train loss 2.8191, val loss 2.8319\n",
      "The current learning rate: 0.00025\n",
      "Epoch 63500: train loss 2.8205, val loss 2.8270\n",
      "The current learning rate: 0.00025\n",
      "Epoch 64000: train loss 2.8318, val loss 2.8279\n",
      "The current learning rate: 0.00025\n",
      "Epoch 64500: train loss 2.8157, val loss 2.8182\n",
      "The current learning rate: 0.00025\n",
      "Epoch 65000: train loss 2.8234, val loss 2.8164\n",
      "The current learning rate: 0.00026\n",
      "Epoch 65500: train loss 2.8084, val loss 2.7997\n",
      "The current learning rate: 0.00026\n",
      "Epoch 66000: train loss 2.8109, val loss 2.8113\n",
      "The current learning rate: 0.00026\n",
      "Epoch 66500: train loss 2.7830, val loss 2.7952\n",
      "The current learning rate: 0.00026\n",
      "Epoch 67000: train loss 2.7910, val loss 2.7885\n",
      "The current learning rate: 0.00026\n",
      "Epoch 67500: train loss 2.7732, val loss 2.7773\n",
      "The current learning rate: 0.00027\n",
      "Epoch 68000: train loss 2.7809, val loss 2.7859\n",
      "The current learning rate: 0.00027\n",
      "Epoch 68500: train loss 2.7870, val loss 2.7815\n",
      "The current learning rate: 0.00027\n",
      "Epoch 69000: train loss 2.7624, val loss 2.7713\n",
      "The current learning rate: 0.00027\n",
      "Epoch 69500: train loss 2.7580, val loss 2.7601\n",
      "The current learning rate: 0.00027\n",
      "Epoch 70000: train loss 2.7452, val loss 2.7462\n",
      "The current learning rate: 0.00028\n",
      "Epoch 70500: train loss 2.7535, val loss 2.7389\n",
      "The current learning rate: 0.00028\n",
      "Epoch 71000: train loss 2.7366, val loss 2.7561\n",
      "The current learning rate: 0.00028\n",
      "Epoch 71500: train loss 2.7235, val loss 2.7421\n",
      "The current learning rate: 0.00028\n",
      "Epoch 72000: train loss 2.7457, val loss 2.7457\n",
      "The current learning rate: 0.00029\n",
      "Epoch 72500: train loss 2.7427, val loss 2.7361\n",
      "The current learning rate: 0.00029\n",
      "Epoch 73000: train loss 2.7089, val loss 2.7197\n",
      "The current learning rate: 0.00029\n",
      "Epoch 73500: train loss 2.7050, val loss 2.7088\n",
      "The current learning rate: 0.00029\n",
      "Epoch 74000: train loss 2.7025, val loss 2.7116\n",
      "The current learning rate: 0.00029\n",
      "Epoch 74500: train loss 2.6784, val loss 2.6899\n",
      "The current learning rate: 0.00030\n",
      "Epoch 75000: train loss 2.6916, val loss 2.6975\n",
      "The current learning rate: 0.00030\n",
      "Epoch 75500: train loss 2.6777, val loss 2.6767\n",
      "The current learning rate: 0.00030\n",
      "Epoch 76000: train loss 2.6826, val loss 2.6814\n",
      "The current learning rate: 0.00030\n",
      "Epoch 76500: train loss 2.6741, val loss 2.6805\n",
      "The current learning rate: 0.00030\n",
      "Epoch 77000: train loss 2.6666, val loss 2.6675\n",
      "The current learning rate: 0.00031\n",
      "Epoch 77500: train loss 2.6619, val loss 2.6690\n",
      "The current learning rate: 0.00031\n",
      "Epoch 78000: train loss 2.7080, val loss 2.6972\n",
      "The current learning rate: 0.00031\n",
      "Epoch 78500: train loss 2.6537, val loss 2.6597\n",
      "The current learning rate: 0.00031\n",
      "Epoch 79000: train loss 2.6519, val loss 2.6498\n",
      "The current learning rate: 0.00031\n",
      "Epoch 79500: train loss 2.6468, val loss 2.6371\n",
      "The current learning rate: 0.00032\n",
      "Epoch 80000: train loss 2.6414, val loss 2.6246\n",
      "The current learning rate: 0.00032\n",
      "Epoch 80500: train loss 2.6114, val loss 2.6277\n",
      "The current learning rate: 0.00032\n",
      "Epoch 81000: train loss 2.6602, val loss 2.6627\n",
      "The current learning rate: 0.00032\n",
      "Epoch 81500: train loss 2.6102, val loss 2.6100\n",
      "The current learning rate: 0.00033\n",
      "Epoch 82000: train loss 2.5989, val loss 2.6087\n",
      "The current learning rate: 0.00033\n",
      "Epoch 82500: train loss 2.5945, val loss 2.5936\n",
      "The current learning rate: 0.00033\n",
      "Epoch 83000: train loss 2.5754, val loss 2.5966\n",
      "The current learning rate: 0.00033\n",
      "Epoch 83500: train loss 2.5798, val loss 2.5766\n",
      "The current learning rate: 0.00033\n",
      "Epoch 84000: train loss 2.5802, val loss 2.5805\n",
      "The current learning rate: 0.00034\n",
      "Epoch 84500: train loss 2.5770, val loss 2.5809\n",
      "The current learning rate: 0.00034\n",
      "Epoch 85000: train loss 2.5645, val loss 2.5634\n",
      "The current learning rate: 0.00034\n",
      "Epoch 85500: train loss 2.5611, val loss 2.5662\n",
      "The current learning rate: 0.00034\n",
      "Epoch 86000: train loss 2.5390, val loss 2.5431\n",
      "The current learning rate: 0.00034\n",
      "Epoch 86500: train loss 2.5470, val loss 2.5443\n",
      "The current learning rate: 0.00035\n",
      "Epoch 87000: train loss 2.5351, val loss 2.5220\n",
      "The current learning rate: 0.00035\n",
      "Epoch 87500: train loss 2.5251, val loss 2.5277\n",
      "The current learning rate: 0.00035\n",
      "Epoch 88000: train loss 2.5175, val loss 2.5085\n",
      "The current learning rate: 0.00035\n",
      "Epoch 88500: train loss 2.5191, val loss 2.5355\n",
      "The current learning rate: 0.00035\n",
      "Epoch 89000: train loss 2.5156, val loss 2.5145\n",
      "The current learning rate: 0.00036\n",
      "Epoch 89500: train loss 2.5138, val loss 2.5055\n",
      "The current learning rate: 0.00036\n",
      "Epoch 90000: train loss 2.4864, val loss 2.4906\n",
      "The current learning rate: 0.00036\n",
      "Epoch 90500: train loss 2.4742, val loss 2.4733\n",
      "The current learning rate: 0.00036\n",
      "Epoch 91000: train loss 2.4870, val loss 2.4816\n",
      "The current learning rate: 0.00036\n",
      "Epoch 91500: train loss 2.4619, val loss 2.4687\n",
      "The current learning rate: 0.00037\n",
      "Epoch 92000: train loss 2.4751, val loss 2.4785\n",
      "The current learning rate: 0.00037\n",
      "Epoch 92500: train loss 2.4540, val loss 2.4634\n",
      "The current learning rate: 0.00037\n",
      "Epoch 93000: train loss 2.4525, val loss 2.4468\n",
      "The current learning rate: 0.00037\n",
      "Epoch 93500: train loss 2.4466, val loss 2.4578\n",
      "The current learning rate: 0.00037\n",
      "Epoch 94000: train loss 2.4318, val loss 2.4380\n",
      "The current learning rate: 0.00038\n",
      "Epoch 94500: train loss 2.4328, val loss 2.4273\n",
      "The current learning rate: 0.00038\n",
      "Epoch 95000: train loss 2.4193, val loss 2.4177\n",
      "The current learning rate: 0.00038\n",
      "Epoch 95500: train loss 2.4231, val loss 2.4214\n",
      "The current learning rate: 0.00038\n",
      "Epoch 96000: train loss 2.4059, val loss 2.3992\n",
      "The current learning rate: 0.00038\n",
      "Epoch 96500: train loss 2.3871, val loss 2.4044\n",
      "The current learning rate: 0.00039\n",
      "Epoch 97000: train loss 2.4021, val loss 2.3895\n",
      "The current learning rate: 0.00039\n",
      "Epoch 97500: train loss 2.3846, val loss 2.3897\n",
      "The current learning rate: 0.00039\n",
      "Epoch 98000: train loss 2.3828, val loss 2.3793\n",
      "The current learning rate: 0.00039\n",
      "Epoch 98500: train loss 2.3751, val loss 2.3703\n",
      "The current learning rate: 0.00039\n",
      "Epoch 99000: train loss 2.3590, val loss 2.3738\n",
      "The current learning rate: 0.00040\n",
      "Epoch 99500: train loss 2.3527, val loss 2.3515\n",
      "The current learning rate: 0.00040\n",
      "Epoch 100000: train loss 2.3593, val loss 2.3489\n",
      "The current learning rate: 0.00040\n",
      "Epoch 100500: train loss 2.3496, val loss 2.3438\n",
      "The current learning rate: 0.00040\n",
      "Epoch 101000: train loss 2.3345, val loss 2.3411\n",
      "The current learning rate: 0.00040\n",
      "Epoch 101500: train loss 2.3328, val loss 2.3430\n",
      "The current learning rate: 0.00040\n",
      "Epoch 102000: train loss 2.3309, val loss 2.3330\n",
      "The current learning rate: 0.00041\n",
      "Epoch 102500: train loss 2.3228, val loss 2.3308\n",
      "The current learning rate: 0.00041\n",
      "Epoch 103000: train loss 2.3195, val loss 2.3195\n",
      "The current learning rate: 0.00041\n",
      "Epoch 103500: train loss 2.2923, val loss 2.3062\n",
      "The current learning rate: 0.00041\n",
      "Epoch 104000: train loss 2.3052, val loss 2.3074\n",
      "The current learning rate: 0.00041\n",
      "Epoch 104500: train loss 2.3044, val loss 2.3086\n",
      "The current learning rate: 0.00041\n",
      "Epoch 105000: train loss 2.2815, val loss 2.2886\n",
      "The current learning rate: 0.00042\n",
      "Epoch 105500: train loss 2.2833, val loss 2.2876\n",
      "The current learning rate: 0.00042\n",
      "Epoch 106000: train loss 2.2853, val loss 2.2762\n",
      "The current learning rate: 0.00042\n",
      "Epoch 106500: train loss 2.2854, val loss 2.3006\n",
      "The current learning rate: 0.00042\n",
      "Epoch 107000: train loss 2.2604, val loss 2.2726\n",
      "The current learning rate: 0.00042\n",
      "Epoch 107500: train loss 2.2586, val loss 2.2580\n",
      "The current learning rate: 0.00042\n",
      "Epoch 108000: train loss 2.2656, val loss 2.2645\n",
      "The current learning rate: 0.00043\n",
      "Epoch 108500: train loss 2.2431, val loss 2.2550\n",
      "The current learning rate: 0.00043\n",
      "Epoch 109000: train loss 2.2597, val loss 2.2613\n",
      "The current learning rate: 0.00043\n",
      "Epoch 109500: train loss 2.2382, val loss 2.2425\n",
      "The current learning rate: 0.00043\n",
      "Epoch 110000: train loss 2.2388, val loss 2.2378\n",
      "The current learning rate: 0.00043\n",
      "Epoch 110500: train loss 2.2256, val loss 2.2352\n",
      "The current learning rate: 0.00043\n",
      "Epoch 111000: train loss 2.2283, val loss 2.2360\n",
      "The current learning rate: 0.00044\n",
      "Epoch 111500: train loss 2.2188, val loss 2.2133\n",
      "The current learning rate: 0.00044\n",
      "Epoch 112000: train loss 2.2104, val loss 2.2198\n",
      "The current learning rate: 0.00044\n",
      "Epoch 112500: train loss 2.2042, val loss 2.2176\n",
      "The current learning rate: 0.00044\n",
      "Epoch 113000: train loss 2.2038, val loss 2.2034\n",
      "The current learning rate: 0.00044\n",
      "Epoch 113500: train loss 2.1999, val loss 2.2026\n",
      "The current learning rate: 0.00044\n",
      "Epoch 114000: train loss 2.2024, val loss 2.2086\n",
      "The current learning rate: 0.00045\n",
      "Epoch 114500: train loss 2.1832, val loss 2.1856\n",
      "The current learning rate: 0.00045\n",
      "Epoch 115000: train loss 2.1634, val loss 2.1862\n",
      "The current learning rate: 0.00045\n",
      "Epoch 115500: train loss 2.1812, val loss 2.1836\n",
      "The current learning rate: 0.00045\n",
      "Epoch 116000: train loss 2.1627, val loss 2.1761\n",
      "The current learning rate: 0.00045\n",
      "Epoch 116500: train loss 2.1664, val loss 2.1742\n",
      "The current learning rate: 0.00045\n",
      "Epoch 117000: train loss 2.1633, val loss 2.1566\n",
      "The current learning rate: 0.00045\n",
      "Epoch 117500: train loss 2.1568, val loss 2.1691\n",
      "The current learning rate: 0.00045\n",
      "Epoch 118000: train loss 2.1475, val loss 2.1552\n",
      "The current learning rate: 0.00046\n",
      "Epoch 118500: train loss 2.1448, val loss 2.1545\n",
      "The current learning rate: 0.00046\n",
      "Epoch 119000: train loss 2.1358, val loss 2.1387\n",
      "The current learning rate: 0.00046\n",
      "Epoch 119500: train loss 2.1364, val loss 2.1471\n",
      "The current learning rate: 0.00046\n",
      "Epoch 120000: train loss 2.1302, val loss 2.1418\n",
      "The current learning rate: 0.00046\n",
      "Epoch 120500: train loss 2.1378, val loss 2.1481\n",
      "The current learning rate: 0.00046\n",
      "Epoch 121000: train loss 2.1176, val loss 2.1114\n",
      "The current learning rate: 0.00046\n",
      "Epoch 121500: train loss 2.1314, val loss 2.1221\n",
      "The current learning rate: 0.00046\n",
      "Epoch 122000: train loss 2.1114, val loss 2.1191\n",
      "The current learning rate: 0.00047\n",
      "Epoch 122500: train loss 2.1090, val loss 2.1127\n",
      "The current learning rate: 0.00047\n",
      "Epoch 123000: train loss 2.0979, val loss 2.1134\n",
      "The current learning rate: 0.00047\n",
      "Epoch 123500: train loss 2.0985, val loss 2.1015\n",
      "The current learning rate: 0.00047\n",
      "Epoch 124000: train loss 2.0917, val loss 2.1038\n",
      "The current learning rate: 0.00047\n",
      "Epoch 124500: train loss 2.0865, val loss 2.0882\n",
      "The current learning rate: 0.00047\n",
      "Epoch 125000: train loss 2.0820, val loss 2.1024\n",
      "The current learning rate: 0.00047\n",
      "Epoch 125500: train loss 2.0773, val loss 2.0912\n",
      "The current learning rate: 0.00047\n",
      "Epoch 126000: train loss 2.0712, val loss 2.0808\n",
      "The current learning rate: 0.00047\n",
      "Epoch 126500: train loss 2.0681, val loss 2.0716\n",
      "The current learning rate: 0.00048\n",
      "Epoch 127000: train loss 2.0703, val loss 2.0763\n",
      "The current learning rate: 0.00048\n",
      "Epoch 127500: train loss 2.0511, val loss 2.0700\n",
      "The current learning rate: 0.00048\n",
      "Epoch 128000: train loss 2.0504, val loss 2.0601\n",
      "The current learning rate: 0.00048\n",
      "Epoch 128500: train loss 2.0582, val loss 2.0571\n",
      "The current learning rate: 0.00048\n",
      "Epoch 129000: train loss 2.0382, val loss 2.0550\n",
      "The current learning rate: 0.00048\n",
      "Epoch 129500: train loss 2.0399, val loss 2.0464\n",
      "The current learning rate: 0.00048\n",
      "Epoch 130000: train loss 2.0501, val loss 2.0619\n",
      "The current learning rate: 0.00048\n",
      "Epoch 130500: train loss 2.0377, val loss 2.0468\n",
      "The current learning rate: 0.00048\n",
      "Epoch 131000: train loss 2.0282, val loss 2.0430\n",
      "The current learning rate: 0.00048\n",
      "Epoch 131500: train loss 2.0212, val loss 2.0276\n",
      "The current learning rate: 0.00048\n",
      "Epoch 132000: train loss 2.0279, val loss 2.0466\n",
      "The current learning rate: 0.00049\n",
      "Epoch 132500: train loss 2.0230, val loss 2.0316\n",
      "The current learning rate: 0.00049\n",
      "Epoch 133000: train loss 2.0203, val loss 2.0293\n",
      "The current learning rate: 0.00049\n",
      "Epoch 133500: train loss 2.0164, val loss 2.0304\n",
      "The current learning rate: 0.00049\n",
      "Epoch 134000: train loss 2.0100, val loss 2.0084\n",
      "The current learning rate: 0.00049\n",
      "Epoch 134500: train loss 2.0043, val loss 2.0068\n",
      "The current learning rate: 0.00049\n",
      "Epoch 135000: train loss 1.9934, val loss 1.9984\n",
      "The current learning rate: 0.00049\n",
      "Epoch 135500: train loss 1.9893, val loss 1.9999\n",
      "The current learning rate: 0.00049\n",
      "Epoch 136000: train loss 1.9966, val loss 2.0072\n",
      "The current learning rate: 0.00049\n",
      "Epoch 136500: train loss 1.9928, val loss 1.9966\n",
      "The current learning rate: 0.00049\n",
      "Epoch 137000: train loss 1.9769, val loss 1.9950\n",
      "The current learning rate: 0.00049\n",
      "Epoch 137500: train loss 1.9816, val loss 1.9953\n",
      "The current learning rate: 0.00049\n",
      "Epoch 138000: train loss 1.9725, val loss 1.9888\n",
      "The current learning rate: 0.00049\n",
      "Epoch 138500: train loss 1.9611, val loss 1.9887\n",
      "The current learning rate: 0.00049\n",
      "Epoch 139000: train loss 1.9703, val loss 1.9715\n",
      "The current learning rate: 0.00049\n",
      "Epoch 139500: train loss 1.9583, val loss 1.9759\n",
      "The current learning rate: 0.00050\n",
      "Epoch 140000: train loss 1.9637, val loss 1.9642\n",
      "The current learning rate: 0.00050\n",
      "Epoch 140500: train loss 1.9564, val loss 1.9661\n",
      "The current learning rate: 0.00050\n",
      "Epoch 141000: train loss 1.9582, val loss 1.9645\n",
      "The current learning rate: 0.00050\n",
      "Epoch 141500: train loss 1.9600, val loss 1.9603\n",
      "The current learning rate: 0.00050\n",
      "Epoch 142000: train loss 1.9386, val loss 1.9646\n",
      "The current learning rate: 0.00050\n",
      "Epoch 142500: train loss 1.9430, val loss 1.9533\n",
      "The current learning rate: 0.00050\n",
      "Epoch 143000: train loss 1.9379, val loss 1.9508\n",
      "The current learning rate: 0.00050\n",
      "Epoch 143500: train loss 1.9386, val loss 1.9432\n",
      "The current learning rate: 0.00050\n",
      "Epoch 144000: train loss 1.9346, val loss 1.9437\n",
      "The current learning rate: 0.00050\n",
      "Epoch 144500: train loss 1.9325, val loss 1.9428\n",
      "The current learning rate: 0.00050\n",
      "Epoch 145000: train loss 1.9293, val loss 1.9455\n",
      "The current learning rate: 0.00050\n",
      "Epoch 145500: train loss 1.9305, val loss 1.9390\n",
      "The current learning rate: 0.00050\n",
      "Epoch 146000: train loss 1.9225, val loss 1.9342\n",
      "The current learning rate: 0.00050\n",
      "Epoch 146500: train loss 1.9199, val loss 1.9378\n",
      "The current learning rate: 0.00050\n",
      "Epoch 147000: train loss 1.9237, val loss 1.9297\n",
      "The current learning rate: 0.00050\n",
      "Epoch 147500: train loss 1.9079, val loss 1.9227\n",
      "The current learning rate: 0.00050\n",
      "Epoch 148000: train loss 1.9073, val loss 1.9264\n",
      "The current learning rate: 0.00050\n",
      "Epoch 148500: train loss 1.9054, val loss 1.9227\n",
      "The current learning rate: 0.00050\n",
      "Epoch 149000: train loss 1.9024, val loss 1.9282\n",
      "The current learning rate: 0.00050\n",
      "Epoch 149500: train loss 1.9005, val loss 1.9143\n",
      "The current learning rate: 0.00050\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"qwen3_slm.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr7LjtHcfyiU"
   },
   "source": [
    "# Training Loss Visualization\n",
    "\n",
    "This section creates a plot showing the training progress over time:\n",
    "\n",
    "## **What it does:**\n",
    "- **Loss conversion**: Converts tensor losses to CPU for plotting\n",
    "- **Dual plot**: Shows both training and validation loss curves\n",
    "- **Color coding**: Green for training loss, red for validation loss\n",
    "- **Progress tracking**: X-axis shows evaluation steps (every 500 epochs)\n",
    "\n",
    "## **Plot Features:**\n",
    "- **Training loss**: Shows model learning progress\n",
    "- **Validation loss**: Indicates generalization performance\n",
    "- **Overfitting detection**: Gap between train/val loss indicates overfitting\n",
    "- **Convergence monitoring**: Helps identify when training should stop\n",
    "\n",
    "## **Interpretation:**\n",
    "- **Decreasing loss**: Model is learning effectively\n",
    "- **Converging curves**: Good generalization\n",
    "- **Diverging curves**: Possible overfitting\n",
    "- **Plateau**: Training may be complete\n",
    "\n",
    "## **Purpose:**\n",
    "Visualizes training progress to monitor model performance and detect training issues.\n",
    "\n",
    "> Add blockquote\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "UFxrwQ-gR1Ya",
    "outputId": "f2b1e789-c25c-4592-e49a-4e19d9668672"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "plt.xlabel(\"Steps - Every 500 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16fEEwqLf84b"
   },
   "source": [
    "# Load Best Model & Generate Text\n",
    "\n",
    "This section loads the trained model and demonstrates text generation capabilities:\n",
    "\n",
    "## **Model Loading:**\n",
    "- **Fresh model**: Creates new model instance with same configuration\n",
    "- **Load weights**: Loads the best saved model (`qwen3_slm.pt`)\n",
    "- **Device placement**: Moves model to appropriate device (GPU/CPU)\n",
    "- **Ready for inference**: Model is now ready for text generation\n",
    "\n",
    "## **Text Generation:**\n",
    "- **Prompt**: \"Once upon a time there was a pumpkin.\"\n",
    "- **Generation length**: 200 new tokens\n",
    "- **Tokenization**: Converts text to token IDs for model input\n",
    "- **Decoding**: Converts generated tokens back to readable text\n",
    "\n",
    "## **Process:**\n",
    "1. **Load model**: Restores best training checkpoint\n",
    "2. **Encode prompt**: Converts text to token sequence\n",
    "3. **Generate**: Model creates continuation of the story\n",
    "4. **Decode output**: Converts tokens back to text\n",
    "5. **Display result**: Shows the complete generated story\n",
    "\n",
    "## **Purpose:**\n",
    "Demonstrates the trained model's ability to generate coherent children's stories from simple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpnkFRMjSE8e",
    "outputId": "99aad860-16d5-47d4-eb28-7894968abeeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin. It was a very big, shiny pumpkin. The pumpkin was so excited because it had never seen something so shiny before.\n",
      "\n",
      "The pumpkin floated in the middle of the pumpkin, and got to know what was inside. It was dark and the pumpkin wanted to find out what was inside, but it was too far away.\n",
      "Then, it heard a voice. It was coming from the corner! It said, \"Who must be fit for a surprise!\" The pumpkin opened its eyes wider and tried to find out. Suddenly, it saw a sparkly door. With its breath and then opened.\n",
      "\n",
      "Inside the staff, it found a giant mountain! It had painted the biggest windows of its days ever. There was a sign of Amy's history to test the entrance. The secret of this story was a reminder of the adventure, different. \n",
      "\n",
      "The unusual pumpkin was now much brighter than ever. The sky it felt so warm that it was able to display its light errands. After a few days of patience and\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and generate text\n",
    "model = Qwen3Model(**QWEN3_CONFIG)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"qwen3_slm.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device)))\n",
    "model = model.to(device)\n",
    "\n",
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "context = torch.tensor(tokenizer.encode(sentence, add_special_tokens=False)).unsqueeze(dim=0).to(device)\n",
    "y = model.generate(context, 200)\n",
    "print(tokenizer.decode(y.squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
